{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast image retrieval\n",
    "\n",
    "In the notebook [01_training_and_evaluation_introduction.ipynb](01_training_and_evaluation_introduction.ipynb) we find the most similar reference image by computing the distances between a query image and *all* reference images using the function `compute_distances()`. While computing the L2 distance between two images is fast, for large datasets of tens of thousands of images this exhaustive brute-force search can be a bottleneck for real-time applications.\n",
    "\n",
    "To speed up image retrieval, this notebook shows how to implement an approximate nearest neighbor method designed to work well for large datasets (N) and high dimensional DNN features (D). For example Ball Tree, a popular approach, which has query time growing as O\\[D\\*log(N)\\], compared to O\\[D\\*N\\] for brute force. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure edits to libraries are loaded and plotting is shown in the notebook.\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular python libraries\n",
    "import sys\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "import scrapbook as sb\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# fast.ai\n",
    "import fastai\n",
    "from fastai.vision import (\n",
    "    load_learner,\n",
    "    accuracy,\n",
    "    cnn_learner,\n",
    "    DatasetType,\n",
    "    ImageList,\n",
    "    imagenet_stats,\n",
    "    models,\n",
    "    partial,\n",
    ")\n",
    "\n",
    "# Computer Vision repository\n",
    "sys.path.extend([\".\", \"../..\"])  # to access the utils_cv library\n",
    "from utils_cv.classification.data import Urls\n",
    "from utils_cv.classification.model import TrainMetricsRecorder\n",
    "from utils_cv.common.data import unzip_url\n",
    "from utils_cv.common.gpu import which_processor, db_num_workers\n",
    "from utils_cv.similarity.data import comparative_set_builder\n",
    "from utils_cv.similarity.metrics import (\n",
    "    compute_distances,\n",
    "    positive_image_ranks,\n",
    "    recall_at_k,\n",
    ")\n",
    "from utils_cv.similarity.model import compute_features, compute_features_learner\n",
    "from utils_cv.similarity.plot import (\n",
    "    plot_comparative_set,\n",
    "    plot_distances,\n",
    "    plot_ranks_distribution,\n",
    "    plot_recalls,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast.ai version = 1.0.48\n",
      "Fast.ai (Torch) is using GPU: Tesla V100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Fast.ai version = {fastai.__version__}\")\n",
    "which_processor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "We start with parameter specifications and data preparation. We use the *Fridge objects* dataset, which is composed of 134 images, divided into 4 classes: can, carton, milk bottle and water bottle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Data location\n",
    "#DATA_PATH = unzip_url(Urls.fridge_objects_path, exist_ok=True)\n",
    "#DATA_PATH = unzip_url(\"https://cvbp.blob.core.windows.net/public/datasets/image_classification/food101Subset.zip\", exist_ok=True)\n",
    "DATA_PATH = \"E:/food-101/images\"\n",
    "\n",
    "# DNN configuration\n",
    "BATCH_SIZE = 16\n",
    "ARCHITECTURE = models.resnet18\n",
    "IM_SIZE = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now build our training data object, and split it to get a certain percentage (here 20%) assigned to a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images into fast.ai's ImageDataBunch object\n",
    "random.seed(642)\n",
    "data = (\n",
    "    ImageList.from_folder(DATA_PATH)\n",
    "    #.split_none()\n",
    "    .split_by_rand_pct(valid_pct=0.8, seed=20)\n",
    "    .label_from_folder()\n",
    "    .transform(size=IM_SIZE)\n",
    "    .databunch(bs=BATCH_SIZE, num_workers = db_num_workers())\n",
    "    .normalize(imagenet_stats)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model\n",
    "\n",
    "In this example we will use a [ResNet18](https://arxiv.org/pdf/1512.03385.pdf) CNN from fast.ai's library which is pre-trained on ImageNet. In practice one will want to load a model here which was trained using the [01_training_and_evaluation_introduction.ipynb](01_training_and_evaluation_introduction.ipynb) notebook using the code:\n",
    "```python\n",
    "    learn = load_learner(\".\", 'image_similarity_01_model')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn = load_learner(\".\", 'image_similarity_01_model')\n",
    "learn = cnn_learner(\n",
    "    data,\n",
    "    ARCHITECTURE,\n",
    "    metrics=[accuracy],\n",
    "    callback_fns=[partial(TrainMetricsRecorder, show_graph=True)],\n",
    "    ps=0 #Leave dropout at zero. Higher values tend to perform significantly worse\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "\n",
    "We use the output of the penultimate layer (ie 512 floating points vector) as our image representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"
     ]
    }
   ],
   "source": [
    "# Use penultimate layer as image representation\n",
    "embedding_layer = learn.model[1][-2] \n",
    "print(embedding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='5401' class='' max='6250', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      86.42% [5401/6250 48:20<07:35]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Compute DNN features for all validation images\n",
    "ref_features = compute_features_learner(data, DatasetType.Valid, learn, embedding_layer)\n",
    "print(\n",
    "    f\"Computed DNN features for {len(list(ref_features))} images, \\\n",
    "each consisting of {len(ref_features[list(ref_features)[0]])} floating point values.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Retrieval Example\n",
    "In the cell below, we demonstrate how to do fast image retrieval using scikit-learn's [NearestNeighbors](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html) object. Note that we normalized the embeddings, and chose \"Euclidean\" distance, to match the \"L2\" distance measure used also in the other notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the DNN feature for the query image\n",
    "query_im_path =  str(data.valid_ds.items[1])\n",
    "query_feature = ref_features[query_im_path]\n",
    "print(f\"Query image path: {query_im_path}\")\n",
    "print(f\"Query feature dimension: {len(query_feature)}\")\n",
    "assert len(query_feature) == 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a nearest neighbor object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize query and all reference features to be of unit length\n",
    "query_feature /= np.linalg.norm(query_feature, 2)\n",
    "ref_features_list = np.array(list(ref_features.values()))\n",
    "ref_features_list /= np.linalg.norm(ref_features_list, axis=1)[:,None]\n",
    "\n",
    "# Build nearest neighbor object using the reference set\n",
    "nn = NearestNeighbors(n_neighbors = 10, algorithm='auto', metric='euclidean').fit(ref_features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the nearest neighbor object for image retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the nearest neighbor object to find all reference images which are similar to the query image\n",
    "query_feature = np.reshape(query_feature, (-1, 512))\n",
    "approx_distances, approx_im_indices = nn.kneighbors(query_feature)\n",
    "\n",
    "# Display the results\n",
    "approx_im_paths = [str(data.valid_ds.items[i]) for i in approx_im_indices[0]]\n",
    "plot_distances(list(zip(approx_im_paths, approx_distances[0])), \n",
    "               num_rows=1, num_cols=8, figsize=(17,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval speed\n",
    "\n",
    "This section compares retrieval times of exhaustive-search versus approximate nearest neighbor search by running the respective algorithms multiple times.  To avoid effects from cashing, a new query image is selected for each run at random.\n",
    "\n",
    "Exhaustive search is fast for the small dataset used in this notebook. However, when using even a modest sized dataset with 5,000 images, exhaustive-search already takes 0.1 seconds per query image. In comparison, approximate-search takes 5 miliseconds, ie. is 20 times faster. For larger datasets, the speed multiplier will be even larger.\n",
    "\n",
    "More speed-gains (however possibly at the loss of retrieval accuracy) can be gained by selecting different parameters for the *NearestNeighbors* object - for more info on this see the scikit-learn site on [Nearest Neighbors](https://scikit-learn.org/stable/modules/neighbors.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDEALLY HERE USE TRAINING SET ALREADY TO DRAW QUERY IMAGES FROM !!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "query_im_path = str(np.random.choice(data.valid_ds.items))\n",
    "query_feature = ref_features[query_im_path]\n",
    "distances = compute_distances(query_feature, ref_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "query_im_path = str(np.random.choice(data.valid_ds.items))\n",
    "query_feature = ref_features[query_im_path]\n",
    "query_feature /= np.linalg.norm(query_feature, 2)\n",
    "query_feature = np.reshape(query_feature, (-1, 512))\n",
    "approx_distances, approx_im_indices = nn.kneighbors(query_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval accuracy\n",
    "\n",
    "Nearest neighbor methods are much faster than brute-force search, however these methods can sometimes be incorrect, ie. return what is deemed to be the most similar image even though there are images with lower L2 distance in the dataset.\n",
    "\n",
    "To measure retrieval accuracy we use brute-force search to find the *true* image with lowest L2 distance, and then compare at what position this image is placed using our nearest neighbor object. Ideally, we would like "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute features for the training set, from which query images will be randomly selected\n",
    "train_features = compute_features_learner(data, DatasetType.Train, learn, embedding_layer)\n",
    "print(\n",
    "    f\"Computed DNN features for the {len(list(ref_features))} training images, \\\n",
    "each consisting of {len(ref_features[list(ref_features)[0]])} floating point values.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "ranks = []\n",
    "for iter in tqdm(range(10)):\n",
    "    \n",
    "    # Get random query image\n",
    "    query_im_path = str(np.random.choice(data.train_ds.items))\n",
    "    query_feature = train_features[query_im_path]\n",
    "    print(f\"Query image path: {query_im_path}\")\n",
    "    assert len(query_feature) == 512\n",
    "\n",
    "    # Find closest match (ie. most similar image) using brute-force search \n",
    "    bf_distances_and_paths = compute_distances(query_feature, ref_features)\n",
    "    bf_distances = [d for (p,d) in bf_distances_and_paths]\n",
    "    bf_closest_match_path = bf_distances_and_paths[np.argmin(bf_distances)][0]\n",
    "\n",
    "    # Find closest match (ie. most similar image) using nearest-neighbor search\n",
    "    query_feature /= np.linalg.norm(query_feature, 2)\n",
    "    query_feature = np.reshape(query_feature, (-1, 512))\n",
    "    approx_distances, approx_im_indices = nn.kneighbors(query_feature)\n",
    "\n",
    "    # Find at what position (ie rank) the brute-force result is within the nearest-neighbor search result\n",
    "    # Best: rank 1. \n",
    "    approx_im_paths = [str(data.valid_ds.items[i]) for i in approx_im_indices[0]]\n",
    "    rank = np.where(np.array(approx_im_paths) == bf_closest_match_path)[0]\n",
    "    assert len(approx_rank) == 1\n",
    "    ranks.append(float(rank)+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse result. Best would be if the approximate nearest neighbor is as good as the brute-force search, ie if the rank is always 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The median rank over {len(ranks)} runs is {np.median(ranks)}, and average rank is {np.mean(ranks)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the distribution of ranks\n",
    "plot_ranks_distribution(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log some outputs using scrapbook which are used during testing to verify correct notebook execution\n",
    "#sb.glue(\"query_feature\", query_feature)\n",
    "#sb.glue(\"reference_features\", reference_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cv)",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
