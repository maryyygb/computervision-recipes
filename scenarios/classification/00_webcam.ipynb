{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart: Web Cam Image Classification\n",
    "\n",
    "\n",
    "Image classification is the canonical computer vision task of determining if an image contains a specific object, feature, or activity. Currently, the best models are based on [convolutional neural networks (CNNs)](https://en.wikipedia.org/wiki/Convolutional_neural_network) and are typically pre-trained on [ImageNet dataset](http://www.image-net.org/). The CNN weights are trained on millions of images and hundreds of object classes.  Implementations are available from many deep neural network frameworks including [CNTK](https://www.microsoft.com/en-us/cognitive-toolkit/features/model-gallery/), [fast.ai](https://docs.fast.ai/vision.models.html#Computer-Vision-models-zoo), [Keras](https://keras.io/applications/), [PyTorch](https://pytorch.org/docs/stable/torchvision/models.html), and [TensorFlow](https://tfhub.dev/s?module-type=image-classification).\n",
    "\n",
    "This notebook shows a simple example of loading a pretrained model to score a webcam stream. Here, we use a [ResNet](https://arxiv.org/abs/1512.03385) model using the `fastai.vision` package.\n",
    "\n",
    "For more details about image classification tasks, including transfer learning, please see our [training introduction notebook](01_training_introduction.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite for Webcam example \n",
    "This notebook assumes you have **a webcam** connected to your machine. If you want to use a remote-VM to run the model and codes while using a local machine for the webcam stream, you can use an SSH tunnel:\n",
    "1. SSH connect to your VM\n",
    "    ```\n",
    "    $ ssh -L 8888:localhost:8888 <user-id@url-to-your-vm>\n",
    "    ```\n",
    "2. Launch a Jupyter session on the VM (with port 8888 which is the default)\n",
    "3. Open `localhost:8888` from your browser on the webcam connected local machine to access the Jupyter notebook running on the VM.\n",
    "\n",
    "We use the `ipywebrtc` module to show the webcam widget in the notebook. Currently, the widget works on **Chrome** and **Firefox**. For more details about the widget, please visit `ipywebrtc` [github](https://github.com/maartenbreddels/ipywebrtc) or [documentation](https://ipywebrtc.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import io\n",
    "import os\n",
    "import time\n",
    "import urllib.request\n",
    "\n",
    "import fastai\n",
    "from fastai.vision.all import *\n",
    "from ipywebrtc import CameraStream, ImageRecorder\n",
    "from ipywidgets import HBox, Label, Layout, Widget\n",
    "from PIL import Image\n",
    "# import scrapbook as sb\n",
    "\n",
    "from utils_cv.common.data import data_path\n",
    "from utils_cv.common.gpu import which_processor\n",
    "from utils_cv.classification.data import imagenet_labels\n",
    "from utils_cv.classification.model import IMAGENET_IM_SIZE, model_to_learner\n",
    "IMAGENET_IM_SIZE = 224\n",
    "\n",
    "print(f\"Fast.ai: {fastai.__version__}\")\n",
    "which_processor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pretrained Model\n",
    "\n",
    "To introduce the computer vision approach with CNNs, we will start with a pretrained `ResNet18` CNN which is a relatively small and fast CNNs architecture. The [reported error rate](https://pytorch.org/docs/stable/torchvision/models.html) of the model on ImageNet is 30.24% for top-1 and 10.92% for top-5 (top n labels considered most probable by the model).\n",
    "\n",
    "This model expects RGB-images as input. To take advantage of the pretrained ResNet model that was trained on imagenet, we need to _normalize_ our image data in the same way. This includes reducing the range of values of RGB from 0-255 to 0-1 and using the normalization mean and std which can be found in [`fastai.vision.imagenet_stats`](https://github.com/fastai/fastai/blob/master/fastai/vision/data.py#L78).\n",
    "\n",
    "The output of the model is a probability distribution of the labeled classes in ImageNet. To convert them into human-readable labels, we utilize the label json file from [Keras](https://github.com/keras-team/keras/blob/master/keras/applications/imagenet_utils.py).\n",
    "\n",
    "**Note:** You can load a different model using `learn = load_learner(path)`. To learn more about model-export and load, see the [fastai doc](https://docs.fast.ai/basic_train.html#Deploying-your-model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = imagenet_labels()\n",
    "print(f\"Num labels = {len(labels)}\")\n",
    "print(f\"{', '.join(labels[:5])}, ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following fast.ai's naming conventions:  A [`Learner`](https://docs.fast.ai/basic_train.html#Learner) is a `Trainer` for `model` using `data` to minimize `loss_func` with optimizer `opt_func`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a pretrained imagenet model into Learner for prediction. \n",
    "# You can load an exported model by learn = load_learner(path) as well.\n",
    "learn = model_to_learner(models.resnet18(weights=ResNet18_Weights.DEFAULT), IMAGENET_IM_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify Images\n",
    "\n",
    "## Image file\n",
    "First, we prepare a coffee mug image to show an example of how to score a single image by using the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download an example image\n",
    "IM_URL = \"https://cvbp-secondary.z19.web.core.windows.net/images/cvbp_cup.jpg\"\n",
    "urllib.request.urlretrieve(IM_URL, os.path.join(data_path(), \"example.jpg\"))\n",
    "\n",
    "im = Image.open(os.path.join(data_path(), \"example.jpg\")).convert(\"RGB\")\n",
    "im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `learn.predict` method, we ask the model to classify this image. The model classifies the image returning the label with the highest probability score. In this case, \"coffee_mug\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.get_preds(dl=[[im]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Use the model to predict the class label\n",
    "_, ind, prob = learn.predict(im)\n",
    "predicted_label = labels[ind]\n",
    "predicted_confidence = prob[ind]\n",
    "print(f\"Predicted label: {predicted_label} (confidence = {predicted_confidence:.2f})\")\n",
    "\n",
    "# Show prediction time. Note the first prediction usually takes longer because of the model loading\n",
    "print(f\"Took {time.time()-start_time} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WebCam Stream\n",
    "\n",
    "Now, we use a WebCam stream for image classification. We use `ipywebrtc` to start a webcam and get the video stream which is sent to the notebook's widget. Note that Jupyter widgets are quite unstable - if the widget below does not show then see the \"Troubleshooting\" section in this [FAQ](./FAQ.md) for possible fixes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Webcam\n",
    "w_cam = CameraStream(\n",
    "    constraints={\n",
    "        'facing_mode': 'user',\n",
    "        'audio': False,\n",
    "        'video': { 'width': IMAGENET_IM_SIZE, 'height': IMAGENET_IM_SIZE }\n",
    "    },\n",
    "    layout=Layout(width=f'{IMAGENET_IM_SIZE}px')\n",
    ")\n",
    "# Image recorder for taking a snapshot\n",
    "w_imrecorder = ImageRecorder(stream=w_cam, layout=Layout(padding='0 0 0 50px'))\n",
    "# Label widget to show our classification results\n",
    "w_label = Label(layout=Layout(padding='0 0 0 50px'))\n",
    "\n",
    "def classify_frame(_):\n",
    "    \"\"\" Classify an image snapshot by using a pretrained model\n",
    "    \"\"\"\n",
    "    # Once capturing started, remove the capture widget since we don't need it anymore\n",
    "    if w_imrecorder.layout.display != 'none':\n",
    "        w_imrecorder.layout.display = 'none'\n",
    "        \n",
    "    try:\n",
    "        im = open_image(io.BytesIO(w_imrecorder.image.value), convert_mode='RGB')\n",
    "        _, ind, prob = learn.predict(im)\n",
    "        # Show result label and confidence\n",
    "        w_label.value = f\"{labels[ind]} ({prob[ind]:.2f})\"\n",
    "    except OSError:\n",
    "        # If im_recorder doesn't have valid image data, skip it. \n",
    "        pass\n",
    "    \n",
    "    # Taking the next snapshot programmatically\n",
    "    w_imrecorder.recording = True\n",
    "\n",
    "# Register classify_frame as a callback. Will be called whenever image.value changes. \n",
    "w_imrecorder.image.observe(classify_frame, 'value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show widgets\n",
    "HBox([w_cam, w_imrecorder, w_label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, click the **capture button** in the widget to start classification. Labels are displayed to show the most probable class along with the confidence predicted by the model for an image snapshot.\n",
    "\n",
    "<img src=\"https://cvbp-secondary.z19.web.core.windows.net/images/cvbp_webcam.png\" width=\"400\" />\n",
    "<center>\n",
    "<i>Example Webcam image</i>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "In this notebook, we used a simple example to demonstrate how to use a pretrained model to classify images. The model is limited to only predict object labels that are part of the ImageNet training samples. From our [training introduction notebook](01_training_introduction.ipynb), you can find how to fine-tune a model to customize the model to detect other objects that you may be interested in finding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Stop the model and webcam\n",
    "Widget.close_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preserve some of the notebook outputs\n",
    "sb.glue(\"predicted_label\", predicted_label)\n",
    "sb.glue(\"predicted_confidence\", float(predicted_confidence))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cv)",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
