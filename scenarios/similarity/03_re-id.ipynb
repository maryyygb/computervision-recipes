{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State-of-the-art image similarity \n",
    "\n",
    "This notebook implements a state-of-the-art approach for image similarity.\n",
    "\n",
    "We showed in the [01_training_and_evaluation_introduction](01_training_and_evaluation_introduction.ipynb) notebook how to train a DNN and use its feature embeddings for image retrieval. In that notebook, the DNN was trained using a standard image classification loss. More accurate models are typically trained explicitly for image similarity using Triplet Learning such as the [FaceNet](https://arxiv.org/pdf/1503.03832.pdf) paper. While triplet-based approaches achieve good accuracies, they are conceptually complex, slower, and more difficult to train/converge due to issue such as how to mine relevant triplets.\n",
    "\n",
    "Instead, we implement the BMVC 2019 paper \"[Classification is a Strong Baseline for Deep Metric Learning](https://arxiv.org/abs/1811.12649)\" which shows that this extra overhead is not necessary. Indeed, by making small changes to standard classification DNNs, the authors achieve results which are comparable or better than the previous state-of-the-art.\n",
    "\n",
    "\n",
    "## Reproducing published results\n",
    "\n",
    "### Datasets\n",
    "\n",
    "Three common benchmark datasets were used to verify the correctness of this notebook, namely [CARS-196](https://ai.stanford.edu/~jkrause/cars/car_dataset.html), [CUB-200-2011](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html), and [SOP](http://cvgl.stanford.edu/projects/lifted_struct/).\n",
    "\n",
    "| Name | #classes  | #images |\n",
    "| ---- | --------- | ------- |\n",
    "| CUB-200-2011 |200| ~12,000 |\n",
    "| CARS-196 | 196   | ~16,000 | \n",
    "| SOP  |22634      | ~120,000|\n",
    "\n",
    "\n",
    "We follow the literature closely to replicate the same train/test splits and the same evaluation protocol as most publications (as described e.g. in this [paper](https://arxiv.org/abs/1511.06452)). For the datasets above, out of the total N classes, all images within the first N/2 classes are used for training and the remaining images are used for evaluation. This is an open-set evaluation setting where all images of a class are either fully assigned to training or to testing.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "Our model matches that of the [paper](https://arxiv.org/abs/1811.12649): ResNet-50 architecture with 224 pixel input resolution and a temperature of 0.05. We train the head and the full DNN for 12 epochs each, with a learning rate of 0.01 and 0.0001 respectively. Similar to the paper, we decrease the learning rate by a factor of 10 for the CUB-200-2011 dataset to avoid overfitting. Note that competitive results can often be achieved using just half the number of epochs or less. All training uses fastai's `fit_one_cycle` policy.\n",
    "\n",
    "\n",
    "### Results\n",
    "\n",
    "As can be seen in the tables below, using this notebook we can re-produce the published accuracies. Our results for the CUB-200-2011 and the SOP datasets are close or even above the numbers in the paper; for CARS-196 however they are a few percentage points lower. It is worth pointing out the significant gain in accuracy for the SOP dataset compared to using the standard image classification loss in the [01_training_and_evaluation_introduction](01_training_and_evaluation_introduction.ipynb) notebook, i.e. from 57% to 80%.\n",
    "\n",
    "\n",
    "Recall@1 using 2048 dimensional features:\n",
    "\n",
    "|               |  CUB-200-2011 | CARS-196 | SOP |\n",
    "| ------------- |  ------------ | -------- | --- |\n",
    "| This notebook |         65%   | 84%      | 81% |\n",
    "| Reported in paper|      65%   | 89%      | 80% | \n",
    "\n",
    "\n",
    "Recall@1 using 512 dimensional features:\n",
    "\n",
    "|               |  CUB-200-2011 | CARS-196 | SOP |\n",
    "| ------------- |  ------------ | -------- | --- |\n",
    "| 01 notebook   |        53%    |    75%   | 57% |\n",
    "| This notebook |        58%    |    78%   | 80% |\n",
    "| Reported in paper|     61%    |    84%   | 78% | \n",
    "\n",
    "Finally, using the 4096 dimensional features from the pooling layer of our original ResNet-50 model, we can get a further boost of up to 2-3% compared to using 2048 dimensions:\n",
    "\n",
    "|               |  CUB-200-2011 | CARS-196 | SOP |\n",
    "| ------------- |  ------------ | -------- | --- |\n",
    "| This notebook |         67%   |    87%   | 81% |\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure edits to libraries are loaded and plotting is shown in the notebook.\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-205c7a1eeefc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mscrapbook\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebugger\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mset_trace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\cv\\lib\\site-packages\\scrapbook\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mversion\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mglue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mread_notebook\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mread_notebooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda\\envs\\cv\\lib\\site-packages\\scrapbook\\api.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpapermill\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miorw\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlist_notebook_files\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNotebook\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mScrapbook\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mscraps\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mScrap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscrap_to_payload\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mschemas\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGLUE_PAYLOAD_FMT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\cv\\lib\\site-packages\\scrapbook\\models.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnbformat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msix\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\cv\\lib\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_libs\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhashtable\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_hashtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_lib\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtslib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_tslib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pragma: no cover\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;31m# hack but overkill to use re\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\cv\\lib\\site-packages\\pandas\\_libs\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# flake8: noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m from .tslibs import (\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mNaT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mNaTType\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\cv\\lib\\site-packages\\pandas\\_libs\\tslibs\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# flake8: noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mconversion\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlocalize_pydatetime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize_date\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mnattype\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNaT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNaTType\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miNaT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_null_datetimelike\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mnp_datetime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOutOfBoundsDatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\tslibs\\c_timestamp.pxd\u001b[0m in \u001b[0;36minit pandas._libs.tslibs.conversion\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\tslibs\\c_timestamp.pyx\u001b[0m in \u001b[0;36minit pandas._libs.tslibs.c_timestamp\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\tslibs\\tzconversion.pyx\u001b[0m in \u001b[0;36minit pandas._libs.tslibs.tzconversion\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\tslibs\\timedeltas.pyx\u001b[0m in \u001b[0;36minit pandas._libs.tslibs.timedeltas\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\tslibs\\offsets.pyx\u001b[0m in \u001b[0;36minit pandas._libs.tslibs.offsets\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\tslibs\\ccalendar.pyx\u001b[0m in \u001b[0;36minit pandas._libs.tslibs.ccalendar\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\tslibs\\strptime.pyx\u001b[0m in \u001b[0;36minit pandas._libs.tslibs.strptime\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\tslibs\\strptime.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.strptime.TimeRE.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\tslibs\\strptime.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.strptime.TimeRE.__seqToRE\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\cv\\lib\\site-packages\\pytz\\lazy.py\u001b[0m in \u001b[0;36m_lazy\u001b[1;34m(self, *args, **kw)\u001b[0m\n\u001b[0;32m     99\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfill_iter\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m                         \u001b[0mlist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_iter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m                         \u001b[1;32mfor\u001b[0m \u001b[0mmethod_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_props\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m                             \u001b[0mdelattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLazyList\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\cv\\lib\\site-packages\\pytz\\__init__.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1102\u001b[0m  'Zulu']\n\u001b[0;32m   1103\u001b[0m all_timezones = LazyList(\n\u001b[1;32m-> 1104\u001b[1;33m         tz for tz in all_timezones if resource_exists(tz))\n\u001b[0m\u001b[0;32m   1105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[0mall_timezones_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLazySet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_timezones\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\cv\\lib\\site-packages\\pytz\\__init__.py\u001b[0m in \u001b[0;36mresource_exists\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[1;34m\"\"\"Return true if the given resource exists\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[0mopen_resource\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\cv\\lib\\site-packages\\pytz\\__init__.py\u001b[0m in \u001b[0;36mopen_resource\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    106\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresource_stream\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresource_stream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'zoneinfo/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Regular python libraries\n",
    "import math, os, random, sys, torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import scrapbook as sb\n",
    "import torch.nn as nn\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "# Fast.ai\n",
    "import fastai\n",
    "from fastai.layers import FlattenedLoss\n",
    "from fastai.vision import (\n",
    "    cnn_learner,\n",
    "    DatasetType,\n",
    "    ImageList,\n",
    "    imagenet_stats,\n",
    "    models,\n",
    ")\n",
    "\n",
    "# Computer Vision repository\n",
    "sys.path.extend([\".\", \"../..\"])  # to access the utils_cv library\n",
    "from utils_cv.common.data import unzip_url\n",
    "from utils_cv.common.gpu import which_processor, db_num_workers\n",
    "from utils_cv.similarity.data import Urls\n",
    "from utils_cv.similarity.metrics import compute_distances\n",
    "from utils_cv.similarity.model import compute_features, compute_features_learner\n",
    "from utils_cv.similarity.plot import plot_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Fast.ai version = {fastai.__version__}\")\n",
    "which_processor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data & Parameters\n",
    "\n",
    "A small dataset is provided to run this notebook and to illustrate how the dataset is structured. The embedding dimension should be set to a value <= 2048 to use the pooling layer suggested in the paper, or to 4096 to use the original ResNet-50 pooling layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Dataset\n",
    "#data_root_dir = unzip_url(Urls.fridge_objects_retrieval_path, exist_ok = True)\n",
    "data_root_dir = \"C:/Users/pabuehle/Desktop/market1501/\"\n",
    "DATA_FINETUNE_PATH = os.path.join(data_root_dir, \"train\")\n",
    "DATA_RANKING_PATH = os.path.join(data_root_dir, \"test\")\n",
    "print(\"Image root directory: {}\".format(data_root_dir))\n",
    "\n",
    "# DNN configuration and learning parameters. Use more epochs to possibly improve accuracy.\n",
    "EPOCHS_HEAD = 12 \n",
    "EPOCHS_BODY = 12\n",
    "HEAD_LEARNING_RATE = 0.01   \n",
    "BODY_LEARNING_RATE = 0.0001 \n",
    "BATCH_SIZE = 32\n",
    "IM_SIZE = (128, 64) # 224\n",
    "DROPOUT = 0 \n",
    "ARCHITECTURE = models.resnet50\n",
    "\n",
    "# Desired embedding dimension. Higher dimensions slow down retrieval but often provide better accuracy.\n",
    "EMBEDDING_DIM = 2048\n",
    "assert EMBEDDING_DIM == 4096 or EMBEDDING_DIM <= 2048 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most images are used for training, and only a small percentage for validation to obtain a rough estimate of the validation loss. We use the standard image augmentations specified by fastai's `get_transforms()` function which includes horizontal flipping, image warping and changing pixel intensities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images into fast.ai's ImageDataBunch object\n",
    "random.seed(642)\n",
    "data_finetune = (\n",
    "    ImageList.from_folder(DATA_FINETUNE_PATH)\n",
    "    .split_by_rand_pct(valid_pct=0.05, seed=20)\n",
    "    .label_from_folder()\n",
    "    .transform(tfms=fastai.vision.transform.get_transforms(), size=IM_SIZE)\n",
    "    .databunch(bs=BATCH_SIZE, num_workers = db_num_workers())\n",
    "    .normalize(imagenet_stats)\n",
    ")\n",
    "\n",
    "print(f\"Data for fine-tuning: {len(data_finetune.train_ds.x)} training images and {len(data_finetune.valid_ds.x)} validation images.\")\n",
    "\n",
    "data_finetune.show_batch(rows=3, figsize=(12, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NormSoftmax layers and loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below implements the NormSoftmax loss and layers from the \"[Classification is a Strong Baseline for Deep Metric Learning](https://arxiv.org/abs/1811.12649)\" paper. Most of the code is taken from the [official repository](https://github.com/azgo14/classification_metric_learning) and only slightly modified to work within the fast.ai framework and to optionally use the 4096 dimensional embedding of the original ResNet-50 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddedFeatureWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    DNN head: pools, down-projects, and normalizes DNN features to be of unit length.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, dropout=0):\n",
    "        super(EmbeddedFeatureWrapper, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout = dropout\n",
    "        if output_dim != 4096:\n",
    "            self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.standardize = nn.LayerNorm(input_dim, elementwise_affine = False)\n",
    "        self.remap = None\n",
    "        if input_dim != output_dim:\n",
    "           self.remap = nn.Linear(input_dim, output_dim, bias = False)\n",
    "        if dropout > 0:\n",
    "            self.dropout = nn.Dropout(dropout) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.output_dim != 4096:\n",
    "            x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.standardize(x)\n",
    "        if self.remap:\n",
    "           x = self.remap(x)\n",
    "        if self.dropout > 0:\n",
    "            x = self.dropout(x)      \n",
    "        x = nn.functional.normalize(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class L2NormalizedLinearLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Apply a linear layer to the input, where the weights are normalized to be of unit length.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(L2NormalizedLinearLayer, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.Tensor(output_dim, input_dim))    \n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        # Initialization from nn.Linear (https://github.com/pytorch/pytorch/blob/v1.0.0/torch/nn/modules/linear.py#L129)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        norm_weight = nn.functional.normalize(self.weight, dim=1)\n",
    "        prediction_logits = nn.functional.linear(x, norm_weight)\n",
    "        return prediction_logits\n",
    "        \n",
    "\n",
    "class NormSoftmaxLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Apply temperature scaling on logits before computing the cross-entropy loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, temperature=0.05):\n",
    "        super(NormSoftmaxLoss, self).__init__() \n",
    "        self.temperature = temperature\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    " \n",
    "    def forward(self, prediction_logits, instance_targets):\n",
    "        loss = self.loss_fn(prediction_logits / self.temperature, instance_targets)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified classification DNN\n",
    "\n",
    "We begin by retrieving a pre-trained [ResNet50](https://arxiv.org/pdf/1512.03385.pdf) CNN from fast.ai's library which was trained on ImageNet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = cnn_learner(\n",
    "    data_finetune,\n",
    "    ARCHITECTURE,\n",
    "    metrics=[],\n",
    "    ps=DROPOUT \n",
    ")\n",
    "\n",
    "print(\"** Original model head **\")\n",
    "print(learn.model[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CNN is then modified to use the suggested \"norm softmax loss\" instead of the default cross-entropy loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default uses the 2048 dimensional pooling layer as implemented in the paper.\n",
    "# Optionally can instead keep the 4096-dimensional pooling layer from the ResNet-50 model.\n",
    "if EMBEDDING_DIM != 4096:\n",
    "    modules = []\n",
    "    pooling_dim = 2048\n",
    "else:\n",
    "    modules = [l for l in learn.model[1][:3]]\n",
    "    pooling_dim = 4096\n",
    "    \n",
    "# Add new layers\n",
    "modules.append(EmbeddedFeatureWrapper(input_dim=pooling_dim, \n",
    "                                      output_dim=EMBEDDING_DIM, \n",
    "                                      dropout=DROPOUT))\n",
    "modules.append(L2NormalizedLinearLayer(input_dim=EMBEDDING_DIM, \n",
    "                                       output_dim=len(data_finetune.classes)))\n",
    "learn.model[1] = nn.Sequential(*modules)\n",
    "\n",
    "# Create new learner object since otherwise the new layers are not updated during backprop \n",
    "learn = fastai.vision.Learner(data_finetune, learn.model)\n",
    "\n",
    "# Update loss function\n",
    "learn.loss_func = FlattenedLoss(NormSoftmaxLoss)\n",
    "\n",
    "print(\"\\n** Edited model head **\")\n",
    "print(learn.model[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run DNN training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the [classification notebooks](https://github.com/microsoft/ComputerVision/tree/master/classification/notebooks) we first refine the head and then the full CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(EPOCHS_HEAD, HEAD_LEARNING_RATE) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now unfreeze all the layers and fine-tuning the model more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(EPOCHS_BODY, BODY_LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now load the ranking set which is used to evaluate image retrieval performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images into fast.ai's ImageDataBunch object\n",
    "data_rank = (\n",
    "    ImageList.from_folder(DATA_RANKING_PATH)\n",
    "    .split_none()\n",
    "    .label_from_folder()\n",
    "    .transform(size=IM_SIZE) \n",
    "    .databunch(bs=BATCH_SIZE, num_workers = db_num_workers())\n",
    "    .normalize(imagenet_stats)\n",
    ")\n",
    "\n",
    "print(f\"Data for retrieval evaluation: {len(data_rank.train_ds.x)} images.\")\n",
    "\n",
    "# Display example images\n",
    "data_rank.show_batch(rows=3, figsize=(12, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following line will allow us to extract the DNN features after running each image through the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute DNN features for all validation images\n",
    "embedding_layer = learn.model[1][-2]\n",
    "dnn_features = compute_features_learner(data_rank, DatasetType.Train, learn, embedding_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Retrieval Example\n",
    "The cell below shows how to find and display the most similar images in the ranking set for a given query image (which we also select from the ranking set). This example is similar to the one shown in the [00_webcam.ipynb](https://github.com/microsoft/ComputerVision/tree/master/similarity/notebooks/00_webcam.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the DNN feature for the query image\n",
    "query_im_path =  str(data_rank.train_ds.items[1])\n",
    "query_feature = dnn_features[query_im_path]\n",
    "print(f\"Query image path: {query_im_path}\")\n",
    "print(f\"Query feature dimension: {len(query_feature)}\")\n",
    "assert len(query_feature) == EMBEDDING_DIM\n",
    "\n",
    "# Compute the distances between the query and all reference images\n",
    "distances = compute_distances(query_feature, dnn_features)\n",
    "plot_distances(distances, num_rows=1, num_cols=6, figsize=(15,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative evaluation\n",
    "\n",
    "Finally, to quantitatively evaluate image retrieval performance, we compute the Recall@1 measure. The implementation below is slow but straight-forward and shows the usage of the `compute_distances()` function.\n",
    "\n",
    "Note that the \"[Classification is a Strong Baseline for Deep Metric Learning](https://arxiv.org/abs/1811.12649)\" paper uses the cosine distance, while we use the faster L2 distance. This is possible since all DNN features are L2-normalized and hence both distance metrics return the same ranking order (see: https://en.wikipedia.org/wiki/Cosine_similarity).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init\n",
    "count = 0\n",
    "labels = data_rank.train_ds.y\n",
    "im_paths = data_rank.train_ds.items\n",
    "assert len(labels) == len(im_paths) == len(dnn_features)\n",
    "\n",
    "# Use a subset of at least 500 images from the ranking set as query images.\n",
    "step = math.ceil(len(im_paths)/500.0)\n",
    "\n",
    "\n",
    "step = step * 10\n",
    "\n",
    "\n",
    "query_indices = range(len(im_paths))[::step]\n",
    "\n",
    "# Loop over all query images\n",
    "for query_index in query_indices:\n",
    "    if query_index % (step*100) == 0:\n",
    "        print(query_index, len(im_paths))\n",
    "\n",
    "    # Get the DNN features of the query image\n",
    "    query_im_path =  str(im_paths[query_index])\n",
    "    query_feature = dnn_features[query_im_path]\n",
    "    \n",
    "    # Compute distance to all images in the gallery set. \n",
    "    distances = compute_distances(query_feature, dnn_features)\n",
    "\n",
    "    # Find the image with smallest distance\n",
    "    min_dist = float('inf')\n",
    "    min_dist_index = None\n",
    "    for index, distance in enumerate(distances):\n",
    "        if index != query_index: #ignore the query image itself\n",
    "            if distance[1] < min_dist:\n",
    "                min_dist = distance[1]\n",
    "                min_dist_index = index\n",
    "\n",
    "    # Count how often the image with smallest distance has the same label as the query\n",
    "    if labels[query_index] == labels[min_dist_index]:\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recallAt1 = 100.0 * count / len(query_indices)\n",
    "print(\"Recall@1 = {:2.2f}\".format(recallAt1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log some outputs using scrapbook which are used during testing to verify correct notebook execution\n",
    "sb.glue(\"recallAt1\", recallAt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import torch\n",
    "import numpy as np\n",
    "#import time\n",
    "import os\n",
    "\n",
    "#######################################################################\n",
    "# Evaluate\n",
    "# Call: evaluate(query_feature[i],query_label[i],query_cam[i],gallery_feature,gallery_label,gallery_cam)\n",
    "def evaluate(qf,ql,qc,gf,gl,gc):\n",
    "    query = qf\n",
    "    score = np.dot(gf,query)\n",
    "    # predict index\n",
    "    index = np.argsort(score)  #from small to large\n",
    "    index = index[::-1]\n",
    "\n",
    "    # good index\n",
    "    query_index = np.argwhere(gl==ql)\n",
    "    camera_index = np.argwhere(gc==qc)\n",
    "\n",
    "    good_index = np.setdiff1d(query_index, camera_index, assume_unique=True)\n",
    "    junk_index1 = np.argwhere(gl==-1)\n",
    "    junk_index2 = np.intersect1d(query_index, camera_index)\n",
    "    junk_index = np.append(junk_index2, junk_index1) #.flatten())\n",
    "    \n",
    "    CMC_tmp = compute_mAP(index, good_index, junk_index)\n",
    "    return CMC_tmp\n",
    "\n",
    "\n",
    "def compute_mAP(index, good_index, junk_index):\n",
    "    ap = 0\n",
    "    cmc = torch.IntTensor(len(index)).zero_()\n",
    "    if good_index.size==0:   # if empty\n",
    "        cmc[0] = -1\n",
    "        return ap,cmc\n",
    "\n",
    "    # remove junk_index\n",
    "    mask = np.in1d(index, junk_index, invert=True)\n",
    "    index = index[mask]\n",
    "\n",
    "    # find good_index index\n",
    "    ngood = len(good_index)\n",
    "    mask = np.in1d(index, good_index)\n",
    "    rows_good = np.argwhere(mask==True)\n",
    "    rows_good = rows_good.flatten()\n",
    "    \n",
    "    cmc[rows_good[0]:] = 1\n",
    "    for i in range(ngood):\n",
    "        d_recall = 1.0/ngood\n",
    "        precision = (i+1)*1.0/(rows_good[i]+1)\n",
    "        if rows_good[i]!=0:\n",
    "            old_precision = i*1.0/rows_good[i]\n",
    "        else:\n",
    "            old_precision=1.0\n",
    "        ap = ap + d_recall*(old_precision + precision)/2\n",
    "\n",
    "    return ap, cmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_QUERY_PATH = os.path.join(data_root_dir, \"query\")\n",
    "\n",
    "# Load images into fast.ai's ImageDataBunch object\n",
    "data_query = (\n",
    "    ImageList.from_folder(DATA_QUERY_PATH)\n",
    "    .split_none()\n",
    "    .label_from_folder()\n",
    "    .transform(size=IM_SIZE) \n",
    "    .databunch(bs=BATCH_SIZE, num_workers = db_num_workers())\n",
    "    .normalize(imagenet_stats)\n",
    ")\n",
    "\n",
    "print(f\"Query dataset: {len(data_query.train_ds.x)} images.\")\n",
    "\n",
    "# Display example images\n",
    "data_query.show_batch(rows=3, figsize=(12, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_features = compute_features_learner(data_query, DatasetType.Train, learn, embedding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the DNN feature for the query image\n",
    "query_im_path =  str(data_query.train_ds.items[2])\n",
    "query_feature = query_features[query_im_path]\n",
    "print(f\"Query image path: {query_im_path}\")\n",
    "print(f\"Query feature dimension: {len(query_feature)}\")\n",
    "assert len(query_feature) == EMBEDDING_DIM\n",
    "\n",
    "# Compute the distances between the query and all reference images\n",
    "distances = compute_distances(query_feature, dnn_features)\n",
    "plot_distances(distances, num_rows=1, num_cols=6, figsize=(15,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_labels = data_query.train_ds.y\n",
    "query_im_paths = data_query.train_ds.items\n",
    "\n",
    "query_feature = np.array(list(query_features.values()))\n",
    "query_label = np.array([query_labels[i].obj for i in range(len(query_labels))])\n",
    "#query_cam = np.random.randint(0, 3, len(query_label))\n",
    "query_cam = np.array([int(os.path.basename(p)[-17]) for p in query_im_paths])\n",
    "assert query_feature.shape[0] == len(query_label) == len(query_cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference data\n",
    "labels = data_rank.train_ds.y\n",
    "im_paths = data_rank.train_ds.items\n",
    "\n",
    "gallery_feature = np.array(list(dnn_features.values()))\n",
    "gallery_label = np.array([labels[i].obj for i in range(len(labels))])\n",
    "#gallery_cam = np.random.randint(0, 3, len(query_label))\n",
    "gallery_cam =  np.array([int(os.path.basename(p)[-17]) for p in im_paths])\n",
    "assert gallery_feature.shape[0] == len(gallery_label) == len(gallery_cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gallery_feature = query_feature\n",
    "# gallery_label = query_label\n",
    "# gallery_cam = query_cam #+10000 #np.array([10000] * len(query_cam))\n",
    "# assert gallery_feature.shape[0] == len(gallery_label) == len(gallery_cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap = 0.0\n",
    "CMC = torch.IntTensor(len(gallery_label)).zero_()\n",
    "\n",
    "for i in range(len(query_label)):\n",
    "    ap_tmp, CMC_tmp = evaluate(query_feature[i],query_label[i],query_cam[i],gallery_feature,gallery_label,gallery_cam)\n",
    "    if CMC_tmp[0]==-1:\n",
    "        continue\n",
    "    CMC = CMC + CMC_tmp\n",
    "    ap += ap_tmp\n",
    "    if i % 100 == 0:\n",
    "        print(i, len(query_label), CMC_tmp[0])\n",
    "    \n",
    "CMC = CMC.float()\n",
    "CMC = CMC/len(query_label) #average CMC\n",
    "print('Rank@1:%f Rank@5:%f Rank@10:%f mAP:%f'%(CMC[0],CMC[4],CMC[9],ap/len(query_label)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "CVPR2017 paper:Zhong Z, Zheng L, Cao D, et al. Re-ranking Person Re-identification with k-reciprocal Encoding[J]. 2017.\n",
    "url:http://openaccess.thecvf.com/content_cvpr_2017/papers/Zhong_Re-Ranking_Person_Re-Identification_CVPR_2017_paper.pdf\n",
    "Matlab version: https://github.com/zhunzhong07/person-re-ranking\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "API\n",
    "q_g_dist: query-gallery distance matrix, numpy array, shape [num_query, num_gallery]\n",
    "q_q_dist: query-query distance matrix, numpy array, shape [num_query, num_query]\n",
    "g_g_dist: gallery-gallery distance matrix, numpy array, shape [num_gallery, num_gallery]\n",
    "k1, k2, lambda_value: parameters, the original paper is (k1=20, k2=6, lambda_value=0.3)\n",
    "Returns:\n",
    "  final_dist: re-ranked distance, numpy array, shape [num_query, num_gallery]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def k_reciprocal_neigh( initial_rank, i, k1):\n",
    "    forward_k_neigh_index = initial_rank[i,:k1+1]\n",
    "    backward_k_neigh_index = initial_rank[forward_k_neigh_index,:k1+1]\n",
    "    fi = np.where(backward_k_neigh_index==i)[0]\n",
    "    return forward_k_neigh_index[fi]\n",
    "\n",
    "def re_ranking(q_g_dist, q_q_dist, g_g_dist, k1=20, k2=6, lambda_value=0.3):\n",
    "    # The following naming, e.g. gallery_num, is different from outer scope.\n",
    "    # Don't care about it.\n",
    "    original_dist = np.concatenate(\n",
    "      [np.concatenate([q_q_dist, q_g_dist], axis=1),\n",
    "       np.concatenate([q_g_dist.T, g_g_dist], axis=1)],\n",
    "      axis=0)\n",
    "    original_dist = 2. - 2 * original_dist   # change the cosine similarity metric to euclidean similarity metric\n",
    "    original_dist = np.power(original_dist, 2).astype(np.float32)\n",
    "    original_dist = np.transpose(1. * original_dist/np.max(original_dist,axis = 0))\n",
    "    V = np.zeros_like(original_dist).astype(np.float32)\n",
    "    #initial_rank = np.argsort(original_dist).astype(np.int32)\n",
    "    # top K1+1\n",
    "    initial_rank = np.argpartition( original_dist, range(1,k1+1) )\n",
    "\n",
    "    query_num = q_g_dist.shape[0]\n",
    "    all_num = original_dist.shape[0]\n",
    "\n",
    "    for i in range(all_num):\n",
    "        # k-reciprocal neighbors\n",
    "        k_reciprocal_index = k_reciprocal_neigh( initial_rank, i, k1)\n",
    "        k_reciprocal_expansion_index = k_reciprocal_index\n",
    "        for j in range(len(k_reciprocal_index)):\n",
    "            candidate = k_reciprocal_index[j]\n",
    "            candidate_k_reciprocal_index = k_reciprocal_neigh( initial_rank, candidate, int(np.around(k1/2)))\n",
    "            if len(np.intersect1d(candidate_k_reciprocal_index,k_reciprocal_index))> 2./3*len(candidate_k_reciprocal_index):\n",
    "                k_reciprocal_expansion_index = np.append(k_reciprocal_expansion_index,candidate_k_reciprocal_index)\n",
    "\n",
    "        k_reciprocal_expansion_index = np.unique(k_reciprocal_expansion_index)\n",
    "        weight = np.exp(-original_dist[i,k_reciprocal_expansion_index])\n",
    "        V[i,k_reciprocal_expansion_index] = 1.*weight/np.sum(weight)\n",
    "\n",
    "    original_dist = original_dist[:query_num,]\n",
    "    if k2 != 1:\n",
    "        V_qe = np.zeros_like(V,dtype=np.float32)\n",
    "        for i in range(all_num):\n",
    "            V_qe[i,:] = np.mean(V[initial_rank[i,:k2],:],axis=0)\n",
    "        V = V_qe\n",
    "        del V_qe\n",
    "    del initial_rank\n",
    "    invIndex = []\n",
    "    for i in range(all_num):\n",
    "        invIndex.append(np.where(V[:,i] != 0)[0])\n",
    "\n",
    "    jaccard_dist = np.zeros_like(original_dist,dtype = np.float32)\n",
    "\n",
    "    for i in range(query_num):\n",
    "        temp_min = np.zeros(shape=[1,all_num],dtype=np.float32)\n",
    "        indNonZero = np.where(V[i,:] != 0)[0]\n",
    "        indImages = []\n",
    "        indImages = [invIndex[ind] for ind in indNonZero]\n",
    "        for j in range(len(indNonZero)):\n",
    "            temp_min[0,indImages[j]] = temp_min[0,indImages[j]]+ np.minimum(V[i,indNonZero[j]],V[indImages[j],indNonZero[j]])\n",
    "        jaccard_dist[i] = 1-temp_min/(2.-temp_min)\n",
    "\n",
    "    final_dist = jaccard_dist*(1-lambda_value) + original_dist*lambda_value\n",
    "    del original_dist\n",
    "    del V\n",
    "    del jaccard_dist\n",
    "    final_dist = final_dist[:query_num,query_num:]\n",
    "    return final_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "#from  re_ranking import re_ranking\n",
    "#######################################################################\n",
    "# Evaluate\n",
    "def evaluate(score,ql,qc,gl,gc):\n",
    "    index = np.argsort(score)  #from small to large\n",
    "    #index = index[::-1]\n",
    "    # good index\n",
    "    query_index = np.argwhere(gl==ql)\n",
    "    camera_index = np.argwhere(gc==qc)\n",
    "\n",
    "    good_index = np.setdiff1d(query_index, camera_index, assume_unique=True)\n",
    "    junk_index1 = np.argwhere(gl==-1)\n",
    "    junk_index2 = np.intersect1d(query_index, camera_index)\n",
    "    junk_index = np.append(junk_index2, junk_index1) #.flatten())\n",
    "    \n",
    "    CMC_tmp = compute_mAP(index, good_index, junk_index)\n",
    "    return CMC_tmp\n",
    "\n",
    "\n",
    "def compute_mAP(index, good_index, junk_index):\n",
    "    ap = 0\n",
    "    cmc = torch.IntTensor(len(index)).zero_()\n",
    "    if good_index.size==0:   # if empty\n",
    "        cmc[0] = -1\n",
    "        return ap,cmc\n",
    "\n",
    "    # remove junk_index\n",
    "    mask = np.in1d(index, junk_index, invert=True)\n",
    "    index = index[mask]\n",
    "\n",
    "    # find good_index index\n",
    "    ngood = len(good_index)\n",
    "    mask = np.in1d(index, good_index)\n",
    "    rows_good = np.argwhere(mask==True)\n",
    "    rows_good = rows_good.flatten()\n",
    "    \n",
    "    cmc[rows_good[0]:] = 1\n",
    "    for i in range(ngood):\n",
    "        d_recall = 1.0/ngood\n",
    "        precision = (i+1)*1.0/(rows_good[i]+1)\n",
    "        if rows_good[i]!=0:\n",
    "            old_precision = i*1.0/rows_good[i]\n",
    "        else:\n",
    "            old_precision=1.0\n",
    "        ap = ap + d_recall*(old_precision + precision)/2\n",
    "\n",
    "    return ap, cmc\n",
    "\n",
    "######################################################################\n",
    "# result = scipy.io.loadmat('pytorch_result.mat')\n",
    "# query_feature = result['query_f']\n",
    "# query_cam = result['query_cam'][0]\n",
    "# query_label = result['query_label'][0]\n",
    "# gallery_feature = result['gallery_f']\n",
    "# gallery_cam = result['gallery_cam'][0]\n",
    "# gallery_label = result['gallery_label'][0]\n",
    "\n",
    "CMC = torch.IntTensor(len(gallery_label)).zero_()\n",
    "ap = 0.0\n",
    "#re-ranking\n",
    "print('calculate initial distance')\n",
    "q_g_dist = np.dot(query_feature, np.transpose(gallery_feature))\n",
    "q_q_dist = np.dot(query_feature, np.transpose(query_feature))\n",
    "g_g_dist = np.dot(gallery_feature, np.transpose(gallery_feature))\n",
    "since = time.time()\n",
    "re_rank = re_ranking(q_g_dist, q_q_dist, g_g_dist, k1=20, k2=6, lambda_value=0.3)\n",
    "time_elapsed = time.time() - since\n",
    "print('Reranking complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "for i in range(len(query_label)):\n",
    "    ap_tmp, CMC_tmp = evaluate(re_rank[i,:],query_label[i],query_cam[i],gallery_label,gallery_cam)\n",
    "    if CMC_tmp[0]==-1:\n",
    "        continue\n",
    "    CMC = CMC + CMC_tmp\n",
    "    ap += ap_tmp\n",
    "    #print(i, CMC_tmp[0])\n",
    "\n",
    "CMC = CMC.float()\n",
    "CMC = CMC/len(query_label) #average CMC\n",
    "print('top1:%f top5:%f top10:%f mAP:%f'%(CMC[0],CMC[4],CMC[9],ap/len(query_label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cv)",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
