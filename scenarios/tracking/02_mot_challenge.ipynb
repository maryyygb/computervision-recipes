{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating a Multi-Object Tracking Model on MOT Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a framework for evaluating [FairMOT](https://github.com/ifzhang/FairMOT) on the [MOT Challenge dataset](https://motchallenge.net/).\n",
    "\n",
    "The MOT Challenge datasets are some of the most common benchmarking datasets for measuring multi-object tracking performance on pedestrian data. They provide distinct datasets every few years; their current offerings include MOT15, MOT16/17, and MOT19/20. These datasets contain various annotated video sequences, each with different tracking difficulties. Additionally, the MOT Challenge provides detections for tracking algorithms without detection components.\n",
    "\n",
    "The goal of this notebook is to reproduce published results on the MOT Challenge using the state-of-the-art FairMOT approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure edits to libraries are loaded and plotting is shown in the notebook.\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TorchVision: 0.4.0a0\n",
      "Torch is using GPU: Tesla K80\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from urllib.parse import urljoin\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from utils_cv.common.data import data_path, download, unzip_url\n",
    "from utils_cv.common.gpu import which_processor, is_windows\n",
    "from utils_cv.tracking.data import Urls\n",
    "from utils_cv.tracking.dataset import TrackingDataset\n",
    "from utils_cv.tracking.model import TrackingLearner\n",
    "\n",
    "# Change matplotlib backend so that plots are shown for windows\n",
    "if is_windows():\n",
    "    plt.switch_backend(\"TkAgg\")\n",
    "\n",
    "print(f\"TorchVision: {torchvision.__version__}\")\n",
    "which_processor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above torchvision command displays your machine's GPUs (if it has any) and the compute that `torch/torchvision` is using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will set some model runtime parameters. Here we specify the default FairMOT model that we will evaluate against the MOT17 dataset. The baseline FairMOT model is downloaded and saved to the `./models/baselines` directory as `all_dla34.pth` by running the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch device: cuda\n"
     ]
    }
   ],
   "source": [
    "CONF_THRES = 0.4\n",
    "TRACK_BUFFER = 30\n",
    "\n",
    "# Downloaded MOT Challenge data path\n",
    "MOT_ROOT_PATH = \"../../data/\"\n",
    "RESULT_ROOT = \"./results\"\n",
    "EXP_NAME = \"MOT_val_all_dla34\"\n",
    "\n",
    "# Pretrained model location\n",
    "MODEL_DIR = unzip_url(Urls.baseline_models_path, \"./models\", exist_ok=True)\n",
    "BASELINE_MODEL = osp.join(MODEL_DIR, \"all_dla34.pth\")\n",
    "\n",
    "# Train on the GPU or on the CPU, if a GPU is not available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using torch device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Dataset Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will download the [MOT17](https://motchallenge.net/data/MOT17.zip) dataset and save it to `MOT_SAVED_PATH`. \n",
    "\n",
    "Note: We extract a subset of MOT17 which is around 2 GB. It may take some time to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data saved to /home/albuendi/computervision-recipes/data/MOT17/train\n",
      "Test data saved to /home/albuendi/computervision-recipes/data/MOT17/test\n"
     ]
    }
   ],
   "source": [
    "mot_path = unzip_url(Urls.mot_challenge_path, MOT_ROOT_PATH, exist_ok=True)\n",
    "mot_train_path = osp.join(mot_path, \"train\")\n",
    "mot_test_path = osp.join(mot_path, \"test\")\n",
    "# seqs_str:  various video sequences subfolder names under MOT challenge data\n",
    "train_seqs = [\n",
    "    \"MOT17-02-SDP\",\n",
    "    \"MOT17-04-SDP\",\n",
    "    \"MOT17-05-SDP\",\n",
    "    \"MOT17-09-SDP\",\n",
    "    \"MOT17-10-SDP\",\n",
    "    \"MOT17-11-SDP\",\n",
    "    \"MOT17-13-SDP\",\n",
    "]\n",
    "test_seqs = [\n",
    "    \"MOT17-01-SDP\",\n",
    "    \"MOT17-03-SDP\",\n",
    "    \"MOT17-06-SDP\",\n",
    "    \"MOT17-07-SDP\",\n",
    "    \"MOT17-08-SDP\",\n",
    "    \"MOT17-12-SDP\",\n",
    "    \"MOT17-14-SDP\",\n",
    "]\n",
    "\n",
    "print(f\"Training data saved to {mot_train_path}\")\n",
    "print(f\"Test data saved to {mot_test_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below initializes and loads the model using the `TrackingLearner` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = TrackingLearner(None, BASELINE_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MOT17 provides ground truth annotations for only the training set, so we will be using the training set for evaluation.\n",
    "\n",
    "To evaluate FairMOT on this dataset, we take advantage of the [py-motmetrics](https://github.com/cheind/py-motmetrics) repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded /home/albuendi/computervision-recipes/scenarios/tracking/models/baselines/all_dla34.pth, epoch 10\n",
      "Saved tracking results to ./results/MOT_val_all_dla34/MOT17-02-SDP.txt\n",
      "Evaluate seq: MOT17-02-SDP\n",
      "loaded /home/albuendi/computervision-recipes/scenarios/tracking/models/baselines/all_dla34.pth, epoch 10\n",
      "Saved tracking results to ./results/MOT_val_all_dla34/MOT17-04-SDP.txt\n",
      "Evaluate seq: MOT17-04-SDP\n",
      "loaded /home/albuendi/computervision-recipes/scenarios/tracking/models/baselines/all_dla34.pth, epoch 10\n",
      "Saved tracking results to ./results/MOT_val_all_dla34/MOT17-05-SDP.txt\n",
      "Evaluate seq: MOT17-05-SDP\n",
      "loaded /home/albuendi/computervision-recipes/scenarios/tracking/models/baselines/all_dla34.pth, epoch 10\n",
      "Saved tracking results to ./results/MOT_val_all_dla34/MOT17-09-SDP.txt\n",
      "Evaluate seq: MOT17-09-SDP\n",
      "loaded /home/albuendi/computervision-recipes/scenarios/tracking/models/baselines/all_dla34.pth, epoch 10\n",
      "Saved tracking results to ./results/MOT_val_all_dla34/MOT17-10-SDP.txt\n",
      "Evaluate seq: MOT17-10-SDP\n",
      "loaded /home/albuendi/computervision-recipes/scenarios/tracking/models/baselines/all_dla34.pth, epoch 10\n",
      "Saved tracking results to ./results/MOT_val_all_dla34/MOT17-11-SDP.txt\n",
      "Evaluate seq: MOT17-11-SDP\n",
      "loaded /home/albuendi/computervision-recipes/scenarios/tracking/models/baselines/all_dla34.pth, epoch 10\n",
      "Saved tracking results to ./results/MOT_val_all_dla34/MOT17-13-SDP.txt\n",
      "Evaluate seq: MOT17-13-SDP\n",
      "              IDF1   IDP   IDR  Rcll  Prcn  GT  MT  PT ML   FP    FN IDs    FM  MOTA  MOTP IDt IDa IDm\n",
      "MOT17-02-SDP 64.3% 77.8% 54.7% 68.6% 97.4%  62  22  31  9  334  5839 181   641 65.8% 0.192 108  36  13\n",
      "MOT17-04-SDP 83.7% 86.2% 81.4% 86.3% 91.4%  83  51  20 12 3884  6520  26   200 78.1% 0.170   4  18   2\n",
      "MOT17-05-SDP 78.0% 85.2% 71.8% 81.0% 96.2% 133  62  60 11  224  1313  70   211 76.8% 0.198  73  29  38\n",
      "MOT17-09-SDP 68.9% 75.2% 63.5% 81.2% 96.3%  26  18   8  0  166   999  56   108 77.1% 0.163  40  12   7\n",
      "MOT17-10-SDP 63.9% 70.9% 58.1% 78.7% 96.0%  57  33  24  0  421  2732 153   424 74.3% 0.213  89  42  14\n",
      "MOT17-11-SDP 85.6% 87.9% 83.4% 90.4% 95.3%  75  52  19  4  424   905  41   130 85.5% 0.156  26  18  12\n",
      "MOT17-13-SDP 75.4% 80.4% 71.0% 83.7% 94.8% 110  72  31  7  531  1893  85   367 78.4% 0.205  69  24  33\n",
      "OVERALL      76.8% 82.4% 72.0% 82.0% 93.9% 546 310 193 43 5984 20201 612  2081 76.1% 0.182 409 179 119\n"
     ]
    }
   ],
   "source": [
    "strsummary = tracker.eval_mot(\n",
    "    conf_thres=CONF_THRES,\n",
    "    track_buffer=TRACK_BUFFER,    \n",
    "    data_root=mot_train_path,\n",
    "    seqs=train_seqs,\n",
    "    result_root=RESULT_ROOT,\n",
    "    exp_name=EXP_NAME,\n",
    ")\n",
    "print(strsummary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluating a model on the testing dataset, the MOT Challenge provides the [MOT evaluation server](https://motchallenge.net/instructions/). Here, a user can upload and submit a `.txt` file of prediction results; the service will return metrics. After uploading our results to the [MOT17 evaluation server](https://motchallenge.net/results/MOT17/?det=Private), we can see a MOTA of 68.5 using the `all_dla34.pth` baseline model.\n",
    "\n",
    "<img src=\"media/mot_results.PNG\" style=\"width: 737.5px;height: 365px\"/>\n",
    "\n",
    "The reported evaluation results from the [FairMOT paper](https://arxiv.org/abs/2004.01888) with the test set are as follows:\n",
    "\n",
    "| Dataset | MOTA   | IDF1 | IDS | MT | ML | FPS |\n",
    "|------|------|------|------|------|------|------|\n",
    "|   MOT16  | 68.7| 70.4| 953| 39.5%| 19.0%| 25.9|\n",
    "|   MOT17  | 67.5| 69.8| 2868| 37.7%| 20.8%| 25.9|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded /home/albuendi/computervision-recipes/scenarios/tracking/models/baselines/all_dla34.pth, epoch 10\n",
      "Saved tracking results to ./results/MOT_val_all_dla34/MOT17-01-SDP.txt\n",
      "loaded /home/albuendi/computervision-recipes/scenarios/tracking/models/baselines/all_dla34.pth, epoch 10\n",
      "Saved tracking results to ./results/MOT_val_all_dla34/MOT17-03-SDP.txt\n",
      "loaded /home/albuendi/computervision-recipes/scenarios/tracking/models/baselines/all_dla34.pth, epoch 10\n",
      "Saved tracking results to ./results/MOT_val_all_dla34/MOT17-06-SDP.txt\n",
      "loaded /home/albuendi/computervision-recipes/scenarios/tracking/models/baselines/all_dla34.pth, epoch 10\n",
      "Saved tracking results to ./results/MOT_val_all_dla34/MOT17-07-SDP.txt\n",
      "loaded /home/albuendi/computervision-recipes/scenarios/tracking/models/baselines/all_dla34.pth, epoch 10\n",
      "Saved tracking results to ./results/MOT_val_all_dla34/MOT17-08-SDP.txt\n",
      "loaded /home/albuendi/computervision-recipes/scenarios/tracking/models/baselines/all_dla34.pth, epoch 10\n",
      "Saved tracking results to ./results/MOT_val_all_dla34/MOT17-12-SDP.txt\n",
      "loaded /home/albuendi/computervision-recipes/scenarios/tracking/models/baselines/all_dla34.pth, epoch 10\n",
      "Saved tracking results to ./results/MOT_val_all_dla34/MOT17-14-SDP.txt\n"
     ]
    }
   ],
   "source": [
    "tracker.eval_mot(\n",
    "    conf_thres=CONF_THRES,\n",
    "    track_buffer=TRACK_BUFFER,    \n",
    "    data_root=mot_test_path,\n",
    "    seqs=test_seqs,\n",
    "    result_root=RESULT_ROOT,\n",
    "    exp_name=EXP_NAME,\n",
    "    run_eval=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cv)",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "356.258px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
