{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R(2+1)D Model on Webcam Stream\n",
    "\n",
    "## Prerequisite for Webcam example\n",
    "This notebook assumes you have a webcam connected to your machine. If you want to use a remote-VM to run the model and codes while using a local machine for the webcam stream, you can use an SSH tunnel:\n",
    "\n",
    "1. SSH connect to your VM:\n",
    "`$ ssh -L 8888:localhost:8888 <user-id@url-to-your-vm>`\n",
    "1. Launch a Jupyter session on the VM (with port 8888 which is the default)\n",
    "1. Open localhost:8888 from your browser on the webcam connected local machine to access the Jupyter notebook running on the VM.\n",
    "\n",
    "We use the `ipywebrtc` module to show the webcam widget in the notebook. Currently, the widget works on Chrome and Firefox. For more details about the widget, please visit [ipywebrtc github](https://github.com/maartenbreddels/ipywebrtc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T16:53:52.228564Z",
     "start_time": "2019-10-21T16:53:52.199597Z"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T16:53:53.038919Z",
     "start_time": "2019-10-21T16:53:52.567511Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "from time import sleep, time\n",
    "from threading import Thread\n",
    "\n",
    "import decord\n",
    "import IPython.display\n",
    "from ipywebrtc import CameraStream, ImageRecorder, VideoStream\n",
    "from ipywidgets import HBox, HTML, Layout, VBox, Widget\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.cuda as cuda\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "from vu.data import KINETICS\n",
    "from vu.models.r2plus1d import R2Plus1D \n",
    "from vu.utils import system_info, transforms_video as transforms\n",
    "\n",
    "system_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-trained Model\n",
    "\n",
    "Load R(2+1)D 34-layer model pre-trained on IG65M and fine-tuned on Kinetics400. There are two versions of the model: 8-frame model and 32-frame model based on the input clip length. The 32-frame model is slower than 8-frame model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = os.path.join(\"data\", \"hmdb51\")\n",
    "VIDEO_DIR = os.path.join(DATA_ROOT, \"videos\")\n",
    "# This split is known as \"split1\"\n",
    "TRAIN_SPLIT = os.path.join(DATA_ROOT, \"hmdb51_vid_train_split_1.txt\")\n",
    "TEST_SPLIT = os.path.join(DATA_ROOT, \"hmdb51_vid_val_split_1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8-frame or 32-frame models\n",
    "MODEL_INPUT_SIZE = 32\n",
    "# 16 for 8-frame model.\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# Model configuration\n",
    "r2plus1d_custom_cfgs = dict(\n",
    "    # HMDB51 dataset spec\n",
    "    num_classes=2,\n",
    "    video_dir=VIDEO_DIR,\n",
    "    train_split=TRAIN_SPLIT,\n",
    "    valid_split=TEST_SPLIT,\n",
    "    # Pre-trained model spec (\"Closer look\" and \"Large-scale\" papers)\n",
    "    base_model='ig65m',\n",
    "    sample_length=MODEL_INPUT_SIZE,     \n",
    "    sample_step=1,        # Frame sampling step\n",
    "    im_scale=128,         # After scaling, the frames will be cropped to (112 x 112)\n",
    "    mean=(0.43216, 0.394666, 0.37645),\n",
    "    std=(0.22803, 0.22145, 0.216989),\n",
    "    random_shift=True,\n",
    "    temporal_jitter_step=2,    # Temporal jitter step in frames (only for training set)\n",
    "    flip_ratio=0.5,\n",
    "    random_crop=True,\n",
    "    video_ext='mp4',\n",
    ")\n",
    "\n",
    "# Training configuration\n",
    "train_cfgs = dict(\n",
    "    mixed_prec=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    grad_steps=2,\n",
    "    lr=0.001,         # 0.001 (\"Closer look\" paper, HMDB51)\n",
    "    momentum=0.95,\n",
    "    warmup_pct=0.3,  # First 30% of the steps will be used for warming-up\n",
    "    lr_decay_factor=0.001,\n",
    "    weight_decay=0.0001,\n",
    "    epochs=48,\n",
    "    model_name='custom',\n",
    "    model_dir=os.path.join(\"checkpoints\", \"ig65m_kinetics\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = R2Plus1D(r2plus1d_custom_cfgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load(model_dir=\"checkpoints\", model_name=\"ig65m_kinetics/custom_021\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = learn.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare class names\n",
    "Since we use Kinetics400 model out of the box, we load its class names. The dataset consists of 400 human actions. For example, the first 20 labels are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"OpeningRackDoor\", \"NoAction\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among them, we will use 50 classes that we are interested in (i.e. the actions make sense to demonstrate in front of the webcam) and ignore other classes by filtering out from the model outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T16:54:17.496422Z",
     "start_time": "2019-10-21T16:54:13.239991Z"
    }
   },
   "outputs": [],
   "source": [
    "if cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Model\n",
    "Here, we use a sliding window classification for action recognition on the continuous webcam stream. We average the last 5 windows results to smoothing out the prediction results. We also reject classes that the score is less than `SCORE_THRESHOLD`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T16:54:22.401416Z",
     "start_time": "2019-10-21T16:54:22.370021Z"
    }
   },
   "outputs": [],
   "source": [
    "SCORE_THRESHOLD = 0.04\n",
    "AVERAGING_SIZE = 5  # Averaging 5 latest clips to make video-level prediction (or smoothing)\n",
    "NUM_CLASSES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FRAMES = 32\n",
    "IM_SCALE = 128    # resize then crop\n",
    "INPUT_SIZE = 112  # input clip size: 3 x NUM_FRAMES x 112 x 112\n",
    "# Normalization\n",
    "MEAN = (0.43216, 0.394666, 0.37645)\n",
    "STD = (0.22803, 0.22145, 0.216989)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T16:54:22.810390Z",
     "start_time": "2019-10-21T16:54:22.762505Z"
    }
   },
   "outputs": [],
   "source": [
    "transform = Compose([\n",
    "    transforms.ToTensorVideo(),\n",
    "    transforms.ResizeVideo(IM_SCALE),\n",
    "    transforms.CenterCropVideo(INPUT_SIZE),\n",
    "    transforms.NormalizeVideo(MEAN, STD)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T16:54:24.189891Z",
     "start_time": "2019-10-21T16:54:24.157778Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(frames, transform, device, model):\n",
    "    clip = torch.from_numpy(np.array(frames))\n",
    "    # Transform frames and append batch dim\n",
    "    sample = torch.unsqueeze(transform(clip), 0)\n",
    "    sample = sample.to(device)\n",
    "    output = model(sample)\n",
    "    scores = nn.functional.softmax(output, dim=1).data.cpu().numpy()[0]\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix: Run on a video file\n",
    "Here, we show how to use the model on a video file. We utilize threading so that the inference does not block the video preview.\n",
    "* Prerequisite - Download HMDB51 video files from [here](http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/#Downloads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T16:55:39.905156Z",
     "start_time": "2019-10-21T16:55:39.871297Z"
    }
   },
   "outputs": [],
   "source": [
    "def _predict_video_frames(window, scores_cache, scores_sum, is_ready):\n",
    "    t = time()\n",
    "    scores = predict(window, transform, device, model)\n",
    "    dur = time() - t\n",
    "    # Averaging scores across clips (dense prediction)\n",
    "    scores_cache.append(scores)\n",
    "    scores_sum += scores\n",
    "    if len(scores_cache) == AVERAGING_SIZE:\n",
    "        scores_avg = scores_sum / AVERAGING_SIZE\n",
    "        top_id_score_dict = {\n",
    "            i: scores_avg[i] for i in (-scores_avg).argpartition(1)[:2]\n",
    "        }\n",
    "        top = {labels[k]: v for k, v in top_id_score_dict.items()}\n",
    "        top = sorted(top.items(), key=lambda kv: -kv[1])\n",
    "        # Plot final results nicely\n",
    "        d_caption.update(IPython.display.HTML(\n",
    "            \"{} fps<p style='font-size:20px'>\".format(1 // dur) + \"<br>\".join([\n",
    "                \"{} ({:.3f})\".format(k, v) for k, v in top\n",
    "            ]) + \"</p>\"\n",
    "        ))\n",
    "        scores_sum -= scores_cache.popleft()\n",
    "    \n",
    "    # Inference done. Ready to run on the next frames.\n",
    "    window.popleft()\n",
    "    is_ready[0] = True\n",
    "\n",
    "def predict_video_frames(video_filepath, d_video, d_caption):\n",
    "    \"\"\"Load video and show frames and inference results on\n",
    "    d_video and d_caption displays\n",
    "    \"\"\"\n",
    "    video_reader = decord.VideoReader(video_filepath)\n",
    "    print(\"Total frames = {}\".format(len(video_reader)))\n",
    "    \n",
    "    is_ready = [True]\n",
    "    window = deque()\n",
    "    scores_cache = deque()\n",
    "    scores_sum = np.zeros(NUM_CLASSES)\n",
    "    while True:\n",
    "        try:\n",
    "            frame = video_reader.next().asnumpy()\n",
    "            if len(frame.shape) != 3:\n",
    "                break\n",
    "            \n",
    "            # Start an inference thread when ready\n",
    "            if is_ready[0]:\n",
    "                window.append(frame)\n",
    "                if len(window) == NUM_FRAMES:\n",
    "                    is_ready[0] = False\n",
    "                    Thread(\n",
    "                        target=_predict_video_frames,\n",
    "                        args=(window, scores_cache, scores_sum, is_ready)\n",
    "                    ).start()\n",
    "                    \n",
    "            # Show video preview\n",
    "            f = io.BytesIO()\n",
    "            im = Image.fromarray(frame)\n",
    "            im.save(f, 'jpeg')\n",
    "\n",
    "            d_video.update(IPython.display.Image(data=f.getvalue()))\n",
    "            sleep(0.03)\n",
    "        except:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T16:55:43.269735Z",
     "start_time": "2019-10-21T16:55:40.470951Z"
    }
   },
   "outputs": [],
   "source": [
    "#video_filepath = os.path.join(\n",
    "#    \"data\",\n",
    "#    \"testvid.mp4\"\n",
    "#)\n",
    "video_filepath = os.path.join(\"data\", \"custom\", \"testvid1 3173 1-7 Cold 2019-08-19_16_26_46_900.mp4\")\n",
    "#video_filepath = os.path.join(\"data\", \"custom\", \"testvid2 3173 8-15 Cold 2019-08-19_16_43_40_350.mp4\")\n",
    "#video_filepath = os.path.join(\"data\", \"custom\", \"testvid3 3173 8-15 Cold 2019-08-19_16_56_18_925.mp4\")\n",
    "#video_filepath = os.path.join(\"data\", \"custom\", \"testvid3 3173 8-15 Cold 2019-08-19_16_43_40_350.mp4\")\n",
    "\n",
    "\n",
    "d_video = IPython.display.display(\"\", display_id=1)\n",
    "d_caption = IPython.display.display(\"Preparing...\", display_id=2)\n",
    "\n",
    "try:\n",
    "    predict_video_frames(video_filepath, d_video, d_caption)\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r2p1d",
   "language": "python",
   "name": "r2p1d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
